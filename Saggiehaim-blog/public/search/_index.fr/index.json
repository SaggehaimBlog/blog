[{"categories":null,"content":" Hi, I’m Saggie, a Cloud Security Solution Director, a blogger and tech trainer. My experience in Microsoft systems, Security, Monitoring and, Networking help me design better projects and solutions.\nI’m motivated by my passion for troubleshooting and Powershell. Eager to keep learning every day. Improving my skill sets to better understand the constraints in every project or the challenges I face every day.\nMy hunger for knowledge and love of challenges, helped me build and execute the Enterprise Monitoring solution in Elbit systems. Where we implemented the Microsoft SCOM, monitored the entire company. This project helped raise the total uptime of the businesses applications. And to find to the root cause for common issues. I also led the troubleshooting team in the company. Helped developers and systems admins find the cause of performance and malfunction issues.\nMeanwhile, I vastly improved the security and productivity of Lumenis LTD IT. Planning and implementing vast projects as changing the entire networking. moving from flat network to full segmentation and implementing Aruba latest Wireless, Switches and NAC solutions supporting 802.1X and GRE Tunneling. Doing a complete overview of the enterprise domain environment, and providing automation solutions such as User-Life Cycle events and more.\nThese days I teach at CyberControl Academy about Operation systems, Networking, and Security Fundamentals. Working at CyberProof, and writing about IT in my blog.\nHope you enjoy my blogging! Feel free to Contact me ","permalink":"//localhost:8080/about/","tags":null,"title":"About"},{"categories":null,"content":"You can contact me to inquire about the following subjects:\nSpeaking and Training You can often find me speaking at companies’ events and small conferences. Often about IT automation, PowerShell, monitoring, and troubleshooting. Besides speaking, I can also conduct training sessions in different levels, from entry levels to advance topics. My Presentations style is combined slides with live demos, you learn the most by trying. I’m trying to avoid the monotonic sessions, hoping to create good interactions with the viewers and getting feedback often.\nContact me for more info.\nConsulting I have experience working with some of the biggest enterprises in Israel. Hence, complex environments and unique situations are not unusual to me. With this in mind, I can help you in the following areas:\nActive Directory Domain Services I can help you set up new environments, analyze your environments, help with configuration, and troubleshoot complex issues in your Active Directory Domain Services.\nPowerShell In addition to that, I can fully automate most of your daily tasks with PowerShell. MonitoringMonitoring is an art, and it easy to fall short in this subject. A successful monitoring solution must build correctly. Let me show you how to turn your unreliable monitoring solution to a life-saving system.\nContact me! Feedback If you have any feedback about my blog or posts, I want to hear!\nPlease do it! ","permalink":"//localhost:8080/contact/","tags":null,"title":"Contact"},{"categories":["Cloud Security","Microsoft Sentinel"],"content":"For those of you who don\u0026rsquo;t know Azure Sentinel, Azure Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. Azure Sentinel delivers intelligent security analytics and threat intelligence across the enterprise, providing a single solution for alert detection, threat visibility, proactive hunting, and threat response.\nThe challenge begins when you have a multi-tenancy environment to monitor, as you need to jump from one tenant to keep up with the incidents. The deployment and maintenance of Azure Sentinel content are time-consuming, and you can\u0026rsquo;t run cross workspace queries between tenants.\nBut fear not, you can use Azure Lighthouse to ease the management and operation.\nWhat is Azure Lighthouse Azure Lighthouse enables cross- and multi-tenant management, allowing for higher automation, scalability, and enhanced governance across resources and tenants.\nWith Azure Lighthouse, you can \u0026ldquo;expose\u0026rdquo; resource groups (or subscriptions) with the resources inside, in our case Azure Sentinel and Log Analytics workspaces from all of your tenants to your central management.\nThere are no additional costs associated with using Azure Lighthouse to manage Azure resources, and any Azure customer or partner can use Azure Lighthouse.\nAzure Lighthouse is also secured by design, as it delegating permissions to Users and/or groups from the central tenant to the resources in other tenants, which means you use the same account and can see and interact only with what exposed to you.\nImage of Azure Sentinel in a multi-tenant environment using Azure Lighthouse Prerequisites Before we can start, there are a few prerequisites that we need to meet:\nThis deployment must be done by a non-guest account in the customer\u0026rsquo;s tenant who has the Owner built-in role for the subscription being onboarded (or which contains the resource groups that are being onboarded) The tenant ID of the service provider\u0026rsquo;s tenant (where you will be managing the customer\u0026rsquo;s resources) The tenant ID of the customer\u0026rsquo;s tenant (which will have resources managed by the service provider) The subscription IDs for each specific subscription in the customer\u0026rsquo;s tenant that will be managed by the service provider (or that contains the resource group(s) that will be managed by the service provider). After fulfilling those prerequisites, we can continue with our guide.\nPlanning our deployment Permissions When deploying Lighthouse, we are delegating permissions to our users. planning before will save us the time later\nRole Create and Run Playbook Create and edit workbooks, analytic rules, and other Azure Sentinel resources Manage incidents (dismiss, assign etc.) View data, incidents, dashboards and other Azure Sentinel resources Azure Sentinel reader \u0026ndash; \u0026ndash; \u0026ndash; X Azure Sentinel responder \u0026ndash; \u0026ndash; X X Azure Sentinel contributor \u0026ndash; X X X Azure Sentinel contributor + Logic App contributor X X X X Role Definitions To define authorizations, you\u0026rsquo;ll need to know the ID values for each user, user group, or service principal in the service provider tenant to which you want to grant access. You\u0026rsquo;ll also need the role definition ID for each built-in role you want to assign. If you don\u0026rsquo;t have them already, you can retrieve them by running the commands below from within PowerShell:\nGet-AzRoleDefinition | select Name, ID To make it easy, here are the ID that we need:\nRole Name Role ID Reader acdd72a7-3385-48ef-bd42-f606fba81ae7 Azure Sentinel Reader 8d289c81-5878-46d4-8554-54e1e3d8b5cb Azure Sentinel Responder 3e150937-b8fe-4cfb-8069-0eaf05ecd056 Azure Sentinel Contributor ab8e14d6-4a74-4a29-9ba8-549422addade Logic App Contributor 87a39d53-fc1b-424a-814c-f7e04687dc9e Contributor b24988ac-6180-42a0-ab88-20f7382dd24c You will use it later in this guide.\nSecurity Groups To make management easier, I recommend using Azure AD security groups for each role. This gives you the flexibility to add or remove individual users to the group that has access so that you don\u0026rsquo;t have to repeat the onboarding process to make user changes. You can assign roles to a service principal, which can be useful for automation scenarios.\nI will use the following groups in this post:\nGroup Name Permissions Description Sentinel_IAC_SG Contributor Group that will hold the Service Account that will run the automation\u0026rsquo;s SOC_L1_SG Azure Sentinel Responder Group for the SOC L1 team, they usually the first to respond to security incidents SOC_L2_SG Azure Sentinel Contributor, Logic App Contributor Group for the SOC L2, the security analysts. They will deal with escalations, build detection rules and more. Executives_SG Azure Sentinel Reader Group for the executives, so they can see the security status of the organization. Optional - Deploy using PowerShell If you want to deploy the templates using PowerShell, you will first need to set up a session to Azure using PowerShell.\nHere how:\nAz Module Azure PowerShell Az module is a PowerShell module for interacting with Azure. Az offers shorter commands, improved stability, and cross-platform support.\nTo install it, you run the following command:\nInstall-Module az -AllowClobber -Scope CurrentUser Connect to Azure with PowerShell You need to get your Tenant ID and Subscription ID from the Azure Portal.\nWith this information, you can use the Connect-AzAccount to create a session with Azure:\n$TenantID = \u0026#39;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#39; $SubscriptionID = \u0026#39;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#39; Connect-AzAccount -TenantId $TenantID -SubscriptionId $SubscriptionID Now you can interact with Azure from PowerShell.\nDeploying Azure Log Analytics and Azure Sentinel If you don\u0026rsquo;t already have Azure Log Analytics and Azure Sentinel deployed, you will need to deploy it.\nI\u0026rsquo;ve created an ARM Template to help you with it:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;workspaces_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, \u0026#34;retentionInDays\u0026#34;: { \u0026#34;defaultValue\u0026#34;: \u0026#34;90\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Location for all resources.\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;solutions_SecurityInsights_name\u0026#34;: \u0026#34;[concat(\u0026#39;SecurityInsights(\u0026#39;,parameters(\u0026#39;workspaces_name\u0026#39;),\u0026#39;)\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;workspaces_name\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.OperationalInsights/workspaces\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-11-01-preview\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;pergb2018\u0026#34; }, \u0026#34;features\u0026#34;: { \u0026#34;searchVersion\u0026#34;: 1 }, \u0026#34;retentionInDays\u0026#34;: \u0026#34;[parameters(\u0026#39;retentionInDays\u0026#39;)]\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.OperationsManagement/solutions\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-11-01-preview\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;solutions_SecurityInsights_name\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;microsoft.operationalinsights/workspaces\u0026#39;, parameters(\u0026#39;workspaces_name\u0026#39;))]\u0026#34; ], \u0026#34;plan\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;solutions_SecurityInsights_name\u0026#39;)]\u0026#34;, \u0026#34;promotionCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;OMSGallery/SecurityInsights\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;workspaceResourceId\u0026#34;: \u0026#34;[resourceId(\u0026#39;microsoft.operationalinsights/workspaces\u0026#39;, parameters(\u0026#39;workspaces_name\u0026#39;))]\u0026#34;, \u0026#34;containedResources\u0026#34;: [ ] } } ] } You can deploy it using either PowerShell or Azure Portal:\nDeploy Using Azure Portal To deploy the template using Azure Portal, first navigate to Azure Portal and sign in.\nThen on the top search bar, search for \u0026ldquo;Deploy a custom template.\u0026rdquo;\nImage of Azure Portal search with a Deploy search, pointing on Deploy a Custom Template On the Custom deployment page, click on Build your own template in the editor\nImage of Azure Portal Custom deployment page Next, you will need to replace the empty template with the template I provided. The template will add the resources we want to deploy.\nImage of Azure Portal editing a template Click save when you finished.\nLast, we need to provide the info for the template. the information we will need to provide is:\nSubscription - The name of the subscription we want to deploy the resources to Resource Group - The name of the Resources group that we want to deploy the resources to (Create new if you didn\u0026rsquo;t created one) Workspaces_name - The name of the Azure Log Analytics workspace Image of Azure Portal with the deployment scope Click on Review + create to deploy the resources.\nDeploy Using PowerShell To deploy the resources using PowerShell, make sure you first have an active session with PowerShell.\nCreating a Resource group With the New-AzResourceGroup you can create a new resource group. Inside the Resource group, we will deploy the Log Analytics Workspace and Sentinel, and this is the Resource group we will delegate.\nEvery resource in Azure requires a location to be deployed to. The location is referring to the Datacenter region. In this Guide, I will use the West Europe region.\nNew-AzResourceGroup -Name \u0026#34;Sentinel-RG\u0026#34; -Location \u0026#34;WestEurope\u0026#34; You will get asked for the Log Analytics Workspace name.\nDeploying the template Because this is an ARM template deployment, We can use the New-AzResourceGroupDeployment cmdlet.\nNew-AzResourceGroupDeployment -ResourceGroupName \u0026#34;Sentinel-RG\u0026#34; -TemplateFile .\\DeploySentinel.json Deploy Azure Lighthouse To deploy Azure Lighthouse, navigate again to \u0026ldquo;Deploy a custom template.\u0026rdquo;\nOn the Custom deployment page, click on Build your own template in the editor.\nNext, you will need to replace the empty template, but this time with the following:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;mspOfferName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specify the name of the offer from the Managed Service Provider\u0026#34; } }, \u0026#34;mspOfferDescription\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Name of the Managed Service Provider offering\u0026#34; } }, \u0026#34;managedByTenantId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specify the tenant id of the Managed Service Provider\u0026#34; } }, \u0026#34;authorizations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specify an array of objects, containing tuples of Azure Active Directory principalId, a Azure roleDefinitionId, and an optional principalIdDisplayName. The roleDefinition specified is granted to the principalId in the provider\u0026#39;s Active Directory and the principalIdDisplayName is visible to customers.\u0026#34; } }, \u0026#34;rgName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;variables\u0026#34;: { \u0026#34;mspRegistrationName\u0026#34;: \u0026#34;[guid(parameters(\u0026#39;mspOfferName\u0026#39;))]\u0026#34;, \u0026#34;mspAssignmentName\u0026#34;: \u0026#34;[guid(parameters(\u0026#39;rgName\u0026#39;))]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ManagedServices/registrationDefinitions\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;mspRegistrationName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;registrationDefinitionName\u0026#34;: \u0026#34;[parameters(\u0026#39;mspOfferName\u0026#39;)]\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;[parameters(\u0026#39;mspOfferDescription\u0026#39;)]\u0026#34;, \u0026#34;managedByTenantId\u0026#34;: \u0026#34;[parameters(\u0026#39;managedByTenantId\u0026#39;)]\u0026#34;, \u0026#34;authorizations\u0026#34;: \u0026#34;[parameters(\u0026#39;authorizations\u0026#39;)]\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-05-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rgAssignment\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;[parameters(\u0026#39;rgName\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.ManagedServices/registrationDefinitions/\u0026#39;, variables(\u0026#39;mspRegistrationName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;:{ \u0026#34;mode\u0026#34;:\u0026#34;Incremental\u0026#34;, \u0026#34;template\u0026#34;:{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: {}, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ManagedServices/registrationAssignments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;mspAssignmentName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;registrationDefinitionId\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.ManagedServices/registrationDefinitions/\u0026#39;, variables(\u0026#39;mspRegistrationName\u0026#39;))]\u0026#34; } } ] } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;mspOfferName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[concat(\u0026#39;Managed by\u0026#39;, \u0026#39; \u0026#39;, parameters(\u0026#39;mspOfferName\u0026#39;))]\u0026#34; }, \u0026#34;authorizations\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;authorizations\u0026#39;)]\u0026#34; } } } click on save when you finished.\nOn the next page, it will ask you to fill in the parameters. Click on Edit parameters.\nImage of Azure Portal editing a template in this screen, you will need to copy the following JSON file:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;mspOfferName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;SOC Central Services\u0026#34; }, \u0026#34;rgName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;Sentinel-RG\u0026#34; }, \u0026#34;mspOfferDescription\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;SOC Central Managed Services\u0026#34; }, \u0026#34;managedByTenantId\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34; }, \u0026#34;authorizations\u0026#34;: { \u0026#34;value\u0026#34;: [ { \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;principalIdDisplayName\u0026#34;: \u0026#34;Sentinel_IAC_SG\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;b24988ac-6180-42a0-ab88-20f7382dd24c\u0026#34; }, { \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;principalIdDisplayName\u0026#34;: \u0026#34;SOC_L1_SG\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;3e150937-b8fe-4cfb-8069-0eaf05ecd056\u0026#34; }, { \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;principalIdDisplayName\u0026#34;: \u0026#34;SOC_L2_SG\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;ab8e14d6-4a74-4a29-9ba8-549422addade\u0026#34; }, { \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;principalIdDisplayName\u0026#34;: \u0026#34;SOC_L2_SG\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;87a39d53-fc1b-424a-814c-f7e04687dc9e\u0026#34; }, { \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;principalIdDisplayName\u0026#34;: \u0026#34;Executives_SG\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;acdd72a7-3385-48ef-bd42-f606fba81ae7\u0026#34; } ] } } } Image of Azure Portal editing a template Click Save when you finish copying the script.\nYou\u0026rsquo;ll be sent back to the previous page, but this time, you will have all the parameters filled in from the template lets have a quick overview, so you will understand what you are deploying:\nImage of Azure Portal editing a template Subscription: The Subscription where we deployed our Log Analytics and Sentinel. Region: The Region where we deployed our Log Analytics and Sentinel Msp Offer Name: The name of the Managed service provider name, you will use it later, choose a good name Msp Offer Description: The description of the Managed service provider Managed By Tenant Id: The Tenant Id you delegate permissions to. Authorizations: The Identities from the Managed services Tenant, and the RBAC roles they will get. Resource Group: The name of the Resource Group we want to delegate permissions to. Now, you can press on Review + Create button. If all validations pass, you\u0026rsquo;ll be able to deploy the template and successfully onboard your resource group.\nImage of Azure Portal editing a template Deploy Using PowerShell This time we will have also provide the TemplateParameterFile. We use the same New-AzResourceGroupDeployment cmdlet.\nNew-AzResourceGroupDeployment -ResourceGroupName \u0026#34;Sentinel-RG\u0026#34; -TemplateFile .\\rgDelegatedResourceManagement.json -TemplateParameterFile .\\rgDelegatedResourceManagement.parameters.json MAKE SURE YOU ARE CONNECTED TO THE RELEVANT TENANT AND SUBSCRIPTION BEFORE DEPLOYMENT!\nVerify your deployment To verify you deployed everything as expected, you have a few things you can do.\nIt can take up to 10 mins after the deployment completed to see the changes\nFrom the manged tenant From the manged tenant, you can navigate to Service providers. In the Service providers, you can navigate to Delegations and see all the delegations in your tenant.\nFrom the central tenant From the central tenant, you can navigate to My Customers. In the My Customers page, you can navigate to Delegations or Customers and see all the delegations in your tenant.\nSummary Azure Lighthouse ease the management of a multi-tenancy environment, and with Microsoft adding a lot of new features for Multiple Azure Sentinel workspaces, Azure Lighthouse is a must.\nAfter reading this post, you will know more about Azure Lighthouse and the benefit of using it. You will know how to plan a successful deployment and, of course, how to deploy it.\n","permalink":"//localhost:8080/posts/deploy-azure-sentinel-to-a-multi-tenancy-environment/deploy-azure-sentinel-to-a-multi-tenancy-environment/","tags":["Microsoft Sentinel","PowerShell","Azure"],"title":"Deploy Azure Sentinel to a multi-tenancy environment"},{"categories":["Cloud Security","Microsoft Sentinel"],"content":"Automate Azure Sentinel Deployment Azure Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. Azure Sentinel delivers intelligent security analytics and threat intelligence across the enterprise, providing a single solution for alert detection, threat visibility, proactive hunting, and threat response.\nLike any other cloud services, you can automate most of the Azure Sentinel deployment and configuration. And in this post, you will learn how to automate the core components of Azure Sentinel.\nPrerequisites Before we start, there are few global prerequisites that you need to meet:\nActive Azure Subscription, if you don\u0026rsquo;t have one, create a free account before you begin. Contributor permissions to the subscription. PowerShell V7, if you don\u0026rsquo;t have it installed, install it from the GitHub Repository . Azure Sentinel Automation tools Bringing the right set of tools to the mission allows you to provide the best solution in the shortest time. Before you begin in your journey, spend some time getting familiar with the following tools:\nPowerShell V7 PowerShell V7 is a cross-platform task automation and configuration management framework, consisting of a command-line shell and scripting language. Make sure you installed it on your system.\nAz Module Azure PowerShell Az module is a PowerShell module for interacting with Azure. Az offers shorter commands, improved stability, and cross-platform support.\nTo install it, you run the following command:\nInstall-Module az -AllowClobber -Scope CurrentUser AzSentinel Module AzSentinel is a module built by Wortel, and it will help us automate a few of the processes.\nYou install the AzSentinel Module with the following command:\nInstall-Module AzSentinel -Scope CurrentUser -Force Splatting In most of the code examples, I use \u0026ldquo;splatting\u0026rdquo; to pass the parameters. Splatting makes your commands shorter and easier to read. You can read more about it here. Connect to Azure with PowerShell You also need to set up a session to Azure from PowerShell, and you can create one with the Az module.\nYou need to get your Tenant ID and Subscription ID from the Azure Portal.\nWith this information, you can use the Connect-AzAccount to create a session with Azure:\n$TenantID = \u0026#39;XXXX-XXXX-XXXX-XXXX-XXXX\u0026#39; $SubscriptionID = \u0026#39;XXXX-XXXX-XXXX-XXXX\u0026#39; Connect-AzAccount -TenantId $TenantID -SubscriptionId $SubscriptionID You can now interact with Azure from PowerShell and start your journey to automate Azure Sentinel.\nA Step by Step To a Fully Automated Deployment Every automation process starts with multiple small automated processes. In this post, you will learn how to provision the following components with PowerShell:\nResource Group Log Analytics Azure Sentinel Saved Queries Hunting Queries Alert Rules Playbooks Workbooks Azure Log Analytics, Azure Sentinel, and Logic Apps are all paid services.\nEach component is a piece in the puzzle that builds a fully up and running Azure Sentinel, ready to monitor every environment.\nResource Group The resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources to deploy, manage, and maintain them as a single entity. With the New-AzResourceGroup, you can create a new resource group.\nEvery resource in Azure requires a deployment location. The location is referring to the Datacenter region. In this guide, I will use the West Europe region.\n$Parms = @{ Name = \u0026#34;Sentinel-RG\u0026#34; Location = \u0026#34;WestEurope\u0026#34; } New-AzResourceGroup @Parms Log Analytics Log Analytics is a service that helps you collect and analyze data generated by resources in your cloud and on-premises environments. It gives you real-time insights using integrated search and custom dashboards to readily analyze millions of records across all of your workloads and servers regardless of their physical location.\nAzure Sentinel run on Log Analytics workspace, and use it to store all security-related data. With that said, Log Analytics is the first resource we need to provision.\nTo create a new Log Analytics workspace, you can use the New-AzOperationalInsightsWorkspace.\n$Parms = @{ ResourceGroupName = \u0026#34;Sentinel-RG\u0026#34; Name = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; Location = \u0026#34;WestEurope\u0026#34; } New-AzOperationalInsightsWorkspace @Parms Azure Sentinel After provisioning Log Analytics, you can continue and on-board Azure Sentinel.\nUse the Set-AzSentinel to provision the Log Analytics Workspace:\n$Parms = @{ SubscriptionId = $SubscriptionID WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; } Set-AzSentinel @Parms Azure Sentinel Saved Queries Until this point, you only provisioned \u0026ldquo;Infrastructure.\u0026rdquo; By enabling Azure Sentinel, you can now start the \u0026ldquo;configuration\u0026rdquo; part, and add content to your Azure Sentinel.\nWhen we talk about SIEM and monitoring big data as an essential skill to have, it is the ability to extract the relevant information from the sea of data.\nIn Sentinel, you use the Kusto Language (KQL). With KQL, you can run queries inside Log Analytics, and write Sentinel Alerts rules, Hunting rules, Workbooks, and more.\nSome queries can be significant and complex, and you don\u0026rsquo;t want to write to them again and again. You can save your time and keep your queries inside Log Analytics and use them on demand.\nYou can organize your saved query inside folders by using the Category switch.\nYou can push saved queries with the New-AzOperationalInsightsSavedSearch command:\n$query = @\u0026#34; // Number of requests // Count the total number of calls across all APIs in the last 24 hours. //Total number of call per resource ApiManagementGatewayLogs | where TimeGenerated \u0026gt; ago(1d) | summarize count(CorrelationId) by _ResourceId \u0026#34;@ $param = @{ ResourceGroupName = \u0026#34;sentinel-rg\u0026#34; WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; SavedSearchId = \u0026#34;NumberofAPICallsPerResource\u0026#34; ## Name of the saved query DisplayName = \u0026#34;Number of API calls per resource\u0026#34; ## The name of the Folder your want to store your saved query Category = \u0026#34;API Managment\u0026#34; Query = $query Version = 1 Force = $true } New-AzOperationalInsightsSavedSearch @param Another method is to use JSON or YAML files to hold the information. This method is the recommended approach. It allows you to manage your content inside a git repository, manage versions, and use it in your automated process.\nHere is an example of a JSON file:\n{ \u0026#34;SavedSearchId\u0026#34;: \u0026#34;NumberofAPICallsPerResource\u0026#34;, \u0026#34;DisplayName\u0026#34;: \u0026#34;Number of API calls per resource\u0026#34;, \u0026#34;Category\u0026#34;: \u0026#34;API Managment\u0026#34;, \u0026#34;Query\u0026#34;: \u0026#34; // Number of requests // Count the total number of calls across all APIs in the last 24 hours. //Total number of call per resource ApiManagementGatewayLogs | where TimeGenerated \u0026gt; ago(1d) | summarize count(CorrelationId) by _ResourceId\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34; } Now you need to adjust the script accordingly:\n$SavedQuery = Get-Content .\\NumberofAPICallsPerResource.json | ConvertFrom-Json $param = @{ ResourceGroupName = \u0026#34;sentinel-rg\u0026#34; WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; SavedSearchId = $SavedQuery.SavedSearchId DisplayName = $SavedQuery.DisplayName Category = $SavedQuery.Category Query = $SavedQuery.Query Version = $SavedQuery.Version Force = $true } New-AzOperationalInsightsSavedSearch @param Hunting Queries Hunting queries help you find suspicious activity in your environment. While many are likely to return legitimate activity or potentially malicious activity, they can guide your hunting. If you are confident with the results after running these queries, you could consider turning some or all of them into Azure Sentinel Analytics to alert on.\nTo can create Hunting rules, you can use the `Import-AzSentinelHuntingRule\u0026rsquo; cmdlet.\nFirst, you create a JSON file containing your hunting rule base on this schema:\n{ \u0026#34;analytics\u0026#34;: [ { \u0026#34;DisplayName\u0026#34;: \u0026#34;Example of Hunting Rule\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;This the description of the query.\u0026#34;, \u0026#34;Query\u0026#34;: \u0026#34; // sample query Syslog | limit 10 \u0026#34;, \u0026#34;Tactics\u0026#34;: [ \u0026#34;Persistence\u0026#34;, \u0026#34;Execution\u0026#34; ] } ] } Now, you can import the Hunting Query into your Azure Sentinel:\n$Parms = @{ WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; SettingsFile = .\\exampleHuntingRule.json } Import-AzSentinelHuntingRule @Parms Alerts Rules Alert rules are queries that defined to trigger incidents. You use them to raise incidents when security incidents happen in your environment. Just like Hunting queries, you store your alerts rules in a JSON file.\n{ \u0026#34;analytics\u0026#34;: [ { \u0026#34;displayName\u0026#34;: \u0026#34;Suspicios activities in Office365\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Rare office operations executed on one or more mailboxes.\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;High\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;query\u0026#34;: \u0026#34;let timeframe = 1d; OfficeActivity\u0026#34;, \u0026#34;queryFrequency\u0026#34;: \u0026#34;5H\u0026#34;, \u0026#34;queryPeriod\u0026#34;: \u0026#34;5H\u0026#34;, \u0026#34;triggerOperator\u0026#34;: \u0026#34;GreaterThan\u0026#34;, \u0026#34;triggerThreshold\u0026#34;: 5, \u0026#34;suppressionDuration\u0026#34;: \u0026#34;6H\u0026#34;, \u0026#34;suppressionEnabled\u0026#34;: false, \u0026#34;tactics\u0026#34;: [ \u0026#34;Persistence\u0026#34;, \u0026#34;LateralMovement\u0026#34;, \u0026#34;Collection\u0026#34; ], \u0026#34;playbookName\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;aggregationKind\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;createIncident\u0026#34;: true, \u0026#34;groupingConfiguration\u0026#34;: { \u0026#34;GroupingConfigurationEnabled\u0026#34;: true, \u0026#34;reopenClosedIncident\u0026#34;: true, \u0026#34;lookbackDuration\u0026#34;: \u0026#34;PT6H\u0026#34;, \u0026#34;entitiesMatchingMethod\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;groupByEntities\u0026#34;: [ \u0026#34;Account\u0026#34;, \u0026#34;Ip\u0026#34;, \u0026#34;Host\u0026#34;, \u0026#34;Url\u0026#34; ] } } ] } You can use the Import-AzSentinelAlertRule to import your Alert Rules:\n$Parms = @{ WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; SettingsFile = .\\exampleAlertRule.json } Import-AzSentinelAlertRule @Parms Playbooks and Workbooks Playbooks use Azure Logic Apps to respond to incidents automatically. Logic Apps are a native resource in ARM, and therefore we can automate its deployment with ARM templates.\nAzure Sentinel allows you to create custom workbooks across your data. Workbooks visualize and monitor the data and provide versatility in creating custom dashboards.\nSame as Playbooks, Workbooks are native resources in Azure and use ARM templates\nYou can find examples to Playbooks and Workbooks on the Azure Sentinel Community repository Because this is an ARM template deployment, you deploy it to the Resource group and not to the Log Analytics Workspace. Use the New-AzResourceGroupDeployment cmdlet to deploy either Workbook or Playbook:\n$Parms = @{ ResourceGroupName = \u0026#34;Sentinel-RG\u0026#34; TemplateFile = .\\exampleTemplate.json } New-AzResourceGroupDeployment @Parms Take into account that the deployment will fail if a workbook with the same name already exists.\nPlan first, Succeed later You\u0026rsquo;ve learned how to provision each component and how to deploy your content, now it\u0026rsquo;s time to prepare the content and learn how to connect securely to Azure to automate the deployment from start to end correctly.\nFolder Structure First, I want to explain about folder structure. When you have different files, I like to organize them in folders, so it\u0026rsquo;s easy to manage them and use them in the automation process. In this case, we have five different resources, so, I recommend the following structure:\nSentinel Automation ├───AlertsRules ├───HuntingRules ├───Playbooks ├───SavedQuery └───Workbooks The example above allows us to match the right files to the right cmdlets. For example, to import all your AlertRules, you can do the following:\n$AlertRules = Get-Item \u0026#34;.\\AlertsRules\\*\u0026#34; -Filter \u0026#39;*.json\u0026#39; foreach ($rule in $AlertRules) { try { $Parms = @{ WorkspaceName = \u0026#34;Saggiehaim-Sentinel-WS\u0026#34; SettingsFile = .\\exampleAlertRule.json SubscriptionId = $SubscriptionId Confirm = $false } Import-AzSentinelAlertRule @Parms } catch { $ErrorMessage = $_.Exception.Message Write-Error \u0026#34;Unable to import Alert Rule: $($ErrorMessage)\u0026#34; } } Connecting Securely To Azure Another important topic is how we authenticate to Azure securely. If you paid attention when you created a session with Azure for the first time, using your credential, it asked you to sign in with one timed password in the Microsoft portal. One time passwords are not the behavior we want when we automate things, as it required human intervention. But this is also the expected behavior from a security point of view, right?\nTo overcome this, you need to use an App Registration Account. If you don\u0026rsquo;t know how to create one, you can follow this guide. A little tip: Keeping the password in plain text in scripts is not so safe, so it’s better to secure it. The best approach is to use a certificate (in the guide, you will learn how to do it). But if you still want to go without a certificate, you can always protect the password. You convert the password to secure string and save it to a file (I recommend changing the ACL for the file).\n$CredsFile = \u0026#34;\u0026lt;Path\u0026gt;\\PasswordFile.txt\u0026#34; Read-Host -AsSecureString | ConvertFrom-SecureString | Out-File $CredsFile Now you can connect to Azure more securely.\n$TenantID = \u0026#39;XXXX-XXXX-XXXX-XXXX-XXXX\u0026#39; $SubscriptionID = \u0026#39;XXXX-XXXX-XXXX-XXXX\u0026#39; $appId = \u0026#39;XXXX-XXXX-XXXX-XXXX\u0026#39; $securePassword = Get-Content $CredsFile | ConvertTo-SecureString $credential = New-Object System.Management.Automation.PSCredential ( $AppId, $securePassword ) $connectAzParams = @{ ServicePrincipal = $true SubscriptionId = $SubscriptionId Tenant = $TenantId Credential = $credential } try { Connect-AzAccount @connectAzParams } catch { $ErrorMessage = $_.Exception.Message Write-Error \u0026#34;Unable to connect to Azure: $($ErrorMessage)\u0026#34; exit } Summary Azure Sentinel is the \u0026ldquo;next-gen\u0026rdquo; SIEM in the cloud, and if you are into security, Mastering Azure Sentinel is a must. Combining the cloud capabilities, with a SIEM that is managed by code, gives you endless possibilities to protect your critical assets. In this post, you have learned about Azure Sentinel, the components, and how to automate a deployment securely. You learned how to sort your content and push it to your Azure Sentinel and manage it with ease. Hopefully, this post will help you start with Azure Sentinel and help automate the deployment and maintenance.\n","permalink":"//localhost:8080/posts/automate-azure-sentinel-deployment/automate-azure-sentinel-deployment/","tags":["Microsoft Sentinel","PowerShell","Azure"],"title":"Automate Azure Sentinel Deployment"},{"categories":["PowerShell","Azure"],"content":"PowerShell is the Automator\u0026rsquo;s best friend. I automate everything with PowerShell, and you can also. When it comes to Azure, I can\u0026rsquo;t remember the last time I created a resource in the Portal. I either use PowerShell or IaC to provision resources or modify them.\nIn this post, you will learn how to set up a session with Azure to start to interact with Azure Resources.\nPrerequisites Before we start, there are few global prerequisites that you need to meet:\nActive Azure Subscription, if you don\u0026rsquo;t have one, create a free account before you begin. Reader permissions to the subscription. PowerShell V7, if you don\u0026rsquo;t have it installed, install it from the GitHub Repository. Getting to know the tools PowerShell V7 PowerShell V7 is a cross-platform task automation and configuration management framework, consisting of a command-line shell and scripting language. Make sure you installed it on your system.\nAz Module Azure PowerShell Az module is a PowerShell module for interacting with Azure. Az offers shorter commands, improved stability, and cross-platform support.\nTo install it, you run the following command:\nInstall-Module az -AllowClobber -Scope CurrentUser Splatting In most of the code examples, I use \u0026ldquo;splatting\u0026rdquo; to pass the parameters. Splatting makes your commands shorter and easier to read. You can read more about it here. Connect to Azure with PowerShell You also need to set up a session to Azure from PowerShell, and you can create one with the Az module.\nYou need to get your Tenant ID and Subscription ID from the Azure Portal.\nWith this information, you can use the Connect-AzAccount to create a session with Azure:\n$TenantID = \u0026#39;XXXX-XXXX-XXXX-XXXX-XXXX\u0026#39; $SubscriptionID = \u0026#39;XXXX-XXXX-XXXX-XXXX\u0026#39; Connect-AzAccount -TenantId $TenantID -SubscriptionId $SubscriptionID You can now interact with Azure from PowerShell and start your journey to automate everything.\n","permalink":"//localhost:8080/posts/how-to-connect-to-azure-with-powershell/how-to-connect-to-azure-with-powershell/","tags":["PowerShell","Azure","Automation"],"title":"How to Connect to Azure With Powershell"},{"categories":["PowerShell","WSL"],"content":"With PowerShell 7, we get the best PowerShell edition, most importantly, it’s cross-platform! While last time we installed the latest PowerShell 7 on a container, sometimes we don’t want just a plain environment with PowerShell, sometimes we want a to add other tools, change system configurations or use different Linux Distros.\nThat is why we use WSL.\nWhat is WSL? WSL or Windows Subsystem for Linux is a feature that lets us run full Linux distributions on top of our Windows OS without the overhead of running virtualization tools.\nPrerequisites In order to enable WSL in your Windows 10, You must run Windows 10 Build 16215 or later.\nEnable WSL First, we need to enable WSL. WSL is an optional feature in Windows and not enabled by default. To enable it, open PowerShell as Administrator and run the following command:\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Enable WSL on Windows Confirm and restart your Computer\nInstalling A Linux Distribution Now, that we enabled WSL on our system, its time to install a Linux Distribution. We can do it by the Microsoft Store or with PowerShell.\nInstall Linux Distribution from Store To install a Linux distribution from Microsoft Store, follow the following steps:\nOpen Microsoft Store Search for Ubuntu Click on Get or Install (if you already had it) Installing Ubuntu for WSL from Microsoft Store Install WSL Linux Distribution With PowerShell Similarly, we can install a Linux distribution with PowerShell (The only way to install it on LTSC edition). To do it, we first need to download the appx, in this example, we will download Ubuntu:\nInvoke-WebRequest -Uri https://aka.ms/wsl-ubuntu -OutFile Ubuntu.appx -UseBasicParsing Now we can use the Add-AppxPackage cmdlet to install our distribution:\nAdd-AppxPackage .\\Ubuntu.appx Using PowerShell to run Add-AppxPackage Cmdlt You can find more Linux distributions on Microsoft Docs Starting Ubuntu with WSL Now, we can finally start our Ubuntu for the first time. If you downloaded it from the Store, you can press on Lunch, if you installed in with PowerShell you can find it under the Start Menu.\nImage of PowerShell Terminal Running Ubuntu on WSL for the first time After WSL finish installing our Ubuntu, it will ask us to set up Username and Password for our root user, go ahead and fill in yours.\nImage of PowerShell Terminal Setting up initial configuration TIP: you can now launch Ubuntu directly from your PowerShell session with wsl.exe\nInstalling PowerShell 7 After all the work, we arrived to the fun part. We now Installing PowerShell 7 on our Ubuntu. There are four steps for this process, so let’s begin.\nfirst, we need to create a folder to store our PowerShell files, we will store it on /usr/share, but feel free to use any path you like.\n# Create Folder sudo mkdir /usr/share/PowerShell # Change Working Dir cd /usr/share/PowerShell Now we will download the latest PowerShell 7 build (RC.2 to this time of writing)\nsudo wget https://github.com/PowerShell/PowerShell/releases/download/v7.0.0-rc.2/powershell-7.0.0-rc.2-linux-x64.tar.gz The file we downloaded is zipped so we need to unzip it, we can do it by running:\nsudo tar xzvf powershell-7.0.0-rc.2-linux-x64.tar.gz The last step is to add this folder to the environmental path. This will allow us to run pwsh from anywhere in our OS and won’t require us to specify the full path. To do so, we need to edit our user profile file, it’s a file named .profile that sit in our #HOME directory.\n# Navigate to home directory cd #HOME # Edit the .profile file nano .profile Add the following line in the end of the file:\nPATH=\u0026#34;$PATH:/usr/share/PowerShell\u0026#34; Image of PowerShell Terminal Adding PowerShell folder to our environmental path All that left now, is to restart WSL, and run pwsh! congrats, you just run PowerShell 7 on Ubuntu inside your Windows 10 device!\nImage of PowerShell Terminal with PowerShell 7 on Ubuntu Clean up Now that we finished installing our PowerShell, we can remove the zipped file by running:\nsudo rm /usr/share/PowerShell/powershell-7.0.0-rc.2-linux-x64.tar.gz Conclusion In this post, we added another tool to our toolbox, as we now know how to install and configure WSL and how to install and set up PowerShell 7 on it. Use it when you want to test your cross-platform scripts or modules, or to explore the differences between each OS.\nI hope you learn a new thing today and enjoyed this post!\n","permalink":"//localhost:8080/posts/install-powershell-7-on-wsl-and-ubuntu/install-powershell-7-on-wsl-and-ubuntu/","tags":["PowerShell","Linux","WSL"],"title":"Install Powershell 7 on Wsl and Ubuntu"},{"categories":["Guide","PowerShell"],"content":"PowerShell can run background jobs with the Start-Job cmdlet. Most of the cmdlet also have -AsJob switch to instantly run the cmdlet as a background job. Each job runs a cmdlet in the background without interacting with the current session. This gives us the ability to perform an action on many targets quicker. This also come with some “downsides”, it’s resource consuming (depend on how many tasks running in the background) as each job is a separated process.\nWith this in mind, another parallel solution was needed. Threadjob module extends the existing PowerShell BackgroundJob, providing faster and lighter resource consuming while running in the current PowerShell session.\nI choose to write about ThreadJob because this is the adopted solution by the PowerShell Team, but you may find other modules doing the same.\nWhy background jobs? By default, PowerShell does synchronous execution. Which means that PowerShell will execute one line of code at a time. With background jobs, we can run different lines of code at the same time (Asynchronous).\nFor the most parts, we’ll run background jobs each time we have repetitive tasks for numerous targets. For example, changing the Active Directory manager attribute for many users, creating a mailbox for list of users, or running the same cmdlet on many servers.\nStart-Job Or Start-ThreadJob? The answer to this question is very simple and straightforward:\nStart-Threadjob – When we want to run tasks in the same context. Has better performance.\nStart-Job – When we need to run something out of the process context. For example, updating PowerShell help in the background. More resources used.\nBackground Jobs with Start-Job The Start-Job cmdlet will start a background job. We can use it in the following way:\n## Start a new Job Start-Job -Name \u0026#34;Test Job\u0026#34; -ScriptBlock {Write-host \u0026#34;This is a Simple Job\u0026#34;} ## Get Job status Get-Job ## Receive Job Get-Job | Receive-Job PowerShell Session Showing Background Job with Start-Job Additionally, as I said in the beginning, Some cmdlet has -Asjob switch. Use Get-Help to check if your cmdlet has the -Asjob switch.\n## Use -AsJob switch to start a new Job Get-WmiObject win32_computersystem model -AsJob ## Get Job status Get-Job ## Receive Job output Get-Job | Receive-Job PowerShell Session Showing Background Job with -AsJob Switch Update: Thanks to @jkavanagh58 who raised the option to find cmdlet with -Asjob switch with the Get-Command cmdlet.\nGet-Command -ParameterName AsJob Background Jobs with Start-ThreadJob While Threadjob doesn’t have -AsThread switch. Everything else is just the same. We use the Start-Threadjob cmdlet and manage the rest same as Start-Job. If you don’t use the latest PowerShell Core, you’ll need to install the module first:\nInstall-Module -Name ThreadJob -Scope CurrentUser Afterwards, we will be able to use the Start-ThreadJob cmdlet:\n## Start a new Thread Start-ThreadJob -Name \u0026#34;Test Thread\u0026#34; -ScriptBlock {Write-Host \u0026#34;This is a Simple Thread Job\u0026#34;} ## Get Job status Get-Job ## Receive Job output Get-Job | Receive-Job PowerShell Session Showing Background Job with Start-ThreadJob Start-Job vs Start-ThreadJob – Performance We learn how to use background jobs and why. Now I want to show you the performance differences.\nMeasure-Command {1..10 | ForEach-Object {sleep 3}} | Select-Object Totalseconds This code will pause the shell for 3 seconds ten times. I’ll run the same code three times with the following conditions:\nAs is with no background jobs With Start-Job With Start-ThreadJob During the tests, we’ll see the difference in each condition. The impact on the operating system and how long it took to execute.\nNo Background jobs Starting with the obvious, the default behavior when we don’t use any background job.\nPowerShell Session Showing measuring a simple iteration with no background jobs. Running ten times and pausing for three seconds each time, set the benchmark to 30 seconds as expected.\nStart-Job Performance Now, running with the native Start-Job, we can see the increased performance as it took only 5.4 seconds to finish.\nPowerShell Session Showing measuring a simple iteration with Process explorer. However, with each iterate, a new PowerShell process started. What will happen if we iterate 1 million times?\nStart-ThreadJob Performance Last, we use Start-Threadjob. This time finishing in 0.245 seconds!\nPowerShell Session Showing measuring a simple iteration with Process explorer. Opposite to the Start-Job method, this time, we stayed in the same process.\nConclusion While using background jobs in the shell is not a common act. It’s not the case when writing scripts. Some times we do care about the performance aspects, and Background jobs can help us. We saw both methods and talked about when we use each method. Now it’s your time to get familiar with the solutions and speed up your scripts!\nHope you enjoyed this post!\n","permalink":"//localhost:8080/posts/background-jobs-start-threadjob-vs-start-job/background-jobs-start-threadjob-vs-start-job/","tags":["PowerShell"],"title":"Background Jobs Start Threadjob vs Start Job"},{"categories":["PowerShell","Docker","Automation"],"content":"With Powershell V7 on the verge, the importance of building solutions that are cross-platforms in Powershell is bigger. One of the issues we have while running tests, is that our Powershell environment is not “clean” as in other computers. That’s where Powershell Core in a container gets in the picture.\nWe install a lot of modules, software and other things that our target audience or servers won’t share. Our solution could act differently in those environments. We also risk not understanding the full dependencies list and limitation of a real environment. That’s why we need a pure Powershell environment.\nWhile installing a new server, whether virtual or physical is a time-consuming task, running a clean Powershell core in a container can take us a few minutes or less.\nWhat are containers anyway? Until containers arrived, we used virtualization to virtualize Operations systems, but, sometimes we need only a small app (like Powershell? 😉) but we don’t want another operating system. We also don’t want to care about OS updates and other dependencies. That is why we “virtualize” apps. You can read more in The Docker site about what are containers.\nPrerequisites Before we can run containers, we need a containers engine. The most popular one is Docker, and in this guide, we will use Docker. The only downside is we can’t use Windows 10 home edition due to Hyper-V limitations.\nHere are the prerequisites:\nWindows 10 64bit: Pro, Enterprise or Education (1607 Anniversary Update, Build 14393 or later). Virtualization is enabled in BIOS. Typically, virtualization is enabled by default. This is different from having Hyper-V enabled. CPU SLAT-capable feature. At least 4GB of RAM. If you meet the prerequisites, go ahead and install the docker engine from docker .\nStarting with containers After installing the Docker engine, we need to pull the image from the Docker hub repository. Docker images contain apps and configuration made by other developers. Microsoft created an image with PowerShell Core .\nPull Docker Image Begin with opening PowerShell. We use the Docker pull command to get the image we want:\ndocker pull mcr.microsoft.com/powershell:latest PowerShell terminal running the Docker pull command Starting a Powershell Core in a container After pulling the image, we can create containers based on the image. To create a container we run the following command:\ndocker run --name ps-core -it mcr.microsoft.com/powershell:latest PowerShell terminal running the Docker Run command Congratulation! you now have a clean Powershell core in a container 😎\nAftermath Finally, when we finish our tests, we need to decide what to do with the container. We want to keep it for future purpose? Or we don’t need it anymore and delete it?\nDelete the container If we decided we don’t need the container anymore, we can remove it easily. First, we need the container ID. We can get it with the command Docker ps.\ndocker ps -a The default behavior for Docker ps command is to show only running containers. We add the -a switch to list both running and stopped containers. Copy the ID of the container, and pass it to the Docker rm command:\ndocker rm \u0026lt;Container ID\u0026gt; PowerShell terminal running the Docker rm command Keep the container If we decided to keep the container, we can re-use it at any time. First, we need to make sure the container is running. We can use the docker ps -a command to verify it. If it’s not running, we can start it with the command Docker start.\ndocker start ps-core Now in order to interact with it, we use the Docker attach command.\ndocker attach ps-core PowerShell terminal running the Docker attach command Conclusion This is only the tip of the iceberg, and containers are a fascinating technology. There is so much to learn and so many things to do with it. But, this is my first step in this world, and hopefully for you too. After this guide, we can easily create a new PowerShell environment to test our cross-platforms solution and better understand the needed dependency for our modules.\nThanks for reading!\n","permalink":"//localhost:8080/posts/running-powershell-core-in-a-container/running-powershell-core-in-a-container/","tags":["PowerShell","Containers","Docker","Automation"],"title":"Running Powershell Core in a Container"},{"categories":["PowerShell","Automation"],"content":"Hey everyone! Today I want to talk about GUI for PowerShell. You probably wonder why? PowerShell is all about the CLI and moving away from the GUI. but sometimes, a GUI for our scripts can be very useful.\nEvery Company has HelpDesk team, other System Admins that don’t know PowerShell at all, or non-technical staff. While I will encourage them to learn, I can’t make them. As a result, I need to provide them with a GUI for some of the tools or solutions.\nOld Lady meme First of all, let’s talk about PowerShell GUI. In fact, There is no such thing “PowerShell GUI” in the PowerShell world, The GUI part is part of the .NET world (which PowerShell built upon). There are Two main ways to provide GUI for our scripts and both of them related to .NET:\nWindows Presentation Foundation – WPF WPF is a subsystem of the .NET framework and can be used for rendering user interfaces in Windows-based applications. Due to the dependency of .NET, WPF is supported only in Windows-based clients.\nWindows Forms – WinForms Similarity to WPF, WinForms is also part of the .NET framework. WinForms is a graphical (GUI) class library included as a part of the Microsoft .NET Framework or Mono Framework, and also supported only in Windows-based clients.\nSoon with .NET Core, we will see solutions for Linux GUI also 😃\nThe task In my company, we use Service Accounts, those accounts are used to run Business Application Services, Tasks and more. They can also be used for developers teams, needing the account for the systems tests run and so. In order to create a Service Account, one shall follow the following guidelines:\nSamAccountName must start with: Srvc Must have a manager (employee who takes responsibility for the account if any anomaly or issue detected) Account must have the following description “Service Account for \u0026lt;Account purpose\u0026gt;” Account office attribute: Must include “Managed by \u0026lt;Employee name\u0026gt; (\u0026lt;Employee ID\u0026gt;) Seems like a easy task right? but when you have a team that needs to do it, a few times a week, it becomes error prone quite fast. So how can we provide a solution?\nThe GUI side I want to provide the team with a tool that will get the information and perform the creation automatically. In addition, I want to send emails for confirmations and audit. First, I like to start with the GUI design, it will make it easier to know what functions I need to build in my script. For small tasks like this, I love to use POSHGUI , it’s a web tool that let you build Winforms GUI for PowerShell in minutes.\nImage of my Service Account creation GUI I won’t include the “Office” attribute as I have all the information needed to update it myself.\nGUI code overview First, we need to Add-Type, this cmdlet adds a .NET class to our Powershell Session. Then we enable visual styles to our form, this adds theme style.\nAdd-Type -AssemblyName System.Windows.Forms [System.Windows.Forms.Application]::EnableVisualStyles() Global configuration Secondly, we need to define global properties, including the form itself, the size of the window, title and more:\n#region begin GUI{ $Form = New-Object system.Windows.Forms.Form # Create a form $Form.ClientSize = \u0026#39;495,216\u0026#39; # Window size $Form.text = \u0026#34;Service Account Creator\u0026#34; # Window title $Form.TopMost = $false #Always on top? $Form.StartPosition = \u0026#34;CenterScreen\u0026#34; #loads the window in the center of the screen $Form.FormBorderStyle = \u0026#39;Fixed3D\u0026#39; #modifies the window border The Service Account Name This ‘TextBox’ will be the SamAccountName of the user, as I said before, it must start with Srvc so I typed it in by default as a reminder.\n$SAccountName = New-Object system.Windows.Forms.TextBox # Declare the Lable $SAccountName.multiline = $false $SAccountName.text = \u0026#34;Srvc\u0026#34; # Default string $SAccountName.width = 309 $SAccountName.height = 20 $SAccountName.location = New-Object System.Drawing.Point(156, 25) $SAccountName.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; $AccountName = New-Object system.Windows.Forms.Label $AccountName.text = \u0026#34;Service Account Name\u0026#34; $AccountName.AutoSize = $true $AccountName.width = 20 $AccountName.height = 10 $AccountName.location = New-Object System.Drawing.Point(18, 25) $AccountName.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; Manager Employee ID In our company the Employee ID is the unique identifier in the Active Directory, we will use it to find the correct employee. we will use it to fill in the information in the service account and to send him a confirmation mail.\n$ManagerEmployee = New-Object system.Windows.Forms.Label $ManagerEmployee.text = \u0026#34;Manger Employee ID\u0026#34; $ManagerEmployee.AutoSize = $true $ManagerEmployee.width = 25 $ManagerEmployee.height = 10 $ManagerEmployee.location = New-Object System.Drawing.Point(18, 55) $ManagerEmployee.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; $ManagerEmployeeID = New-Object system.Windows.Forms.TextBox $ManagerEmployeeID.multiline = $false $ManagerEmployeeID.width = 309 $ManagerEmployeeID.height = 20 $ManagerEmployeeID.location = New-Object System.Drawing.Point(156, 55) $ManagerEmployeeID.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; Creator Employee ID Same as the Manager Employee ID, this time for the creator employee ID, we will use this to audit the process and send him a confirmation mail.\n$CreatorEmployee = New-Object system.Windows.Forms.Label $CreatorEmployee.text = \u0026#34;Creator Employee ID\u0026#34; $CreatorEmployee.AutoSize = $true $CreatorEmployee.width = 25 $CreatorEmployee.height = 10 $CreatorEmployee.location = New-Object System.Drawing.Point(18, 82) $CreatorEmployee.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; $CreatorEmployeeID = New-Object system.Windows.Forms.TextBox $CreatorEmployeeID.multiline = $false $CreatorEmployeeID.width = 309 $CreatorEmployeeID.height = 20 $CreatorEmployeeID.location = New-Object System.Drawing.Point(156, 82) $CreatorEmployeeID.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; Description What is the purpose of this Service account, same as service account name, it must start with “Service Account for ”\n$Description = New-Object system.Windows.Forms.Label $Description.text = \u0026#34;Description\u0026#34; $Description.AutoSize = $true $Description.width = 25 $Description.height = 10 $Description.location = New-Object System.Drawing.Point(18, 110) $Description.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; $TDescription = New-Object system.Windows.Forms.TextBox $TDescription.multiline = $false $TDescription.text = \u0026#34;Service Account For\u0026#34; $TDescription.width = 309 $TDescription.height = 20 $TDescription.location = New-Object System.Drawing.Point(156, 110) $TDescription.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; Create Account button Every program needs an action button. Here we need only one button.\n$Create = New-Object system.Windows.Forms.Button $Create.text = \u0026#34;Create Account\u0026#34; $Create.width = 111 $Create.height = 30 $Create.location = New-Object System.Drawing.Point(178, 154) $Create.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; Status Bar $StatusBar = New-Object System.Windows.Forms.Label $StatusBar.width = 495 $StatusBar.height = 20 $StatusBar.location = New-Object System.Drawing.Point(1, 192) $StatusBar.Font = \u0026#39;Microsoft Sans Serif,10\u0026#39; $StatusBar.Text = \u0026#39;Ready\u0026#39; $StatusBar.Enabled = $true Create the form After we specify each component in our form, we need to invoke it.\n$Form.controls.AddRange(@($SAccountName, $AccountName, $ManagerEmployee, $ManagerEmployeeID, $CreatorEmployee, $CreatorEmployeeID, $Description, $TDescription, $Create, $StatusBar)) Add events After we have the form, we need to add an event, there are plenty events to execute and you can explore them in POSHGUI, here in our example we need to add click action to our button, we will put there the code we want to execute each time we press the button.\n#region gui events { $Create.Add_Click( { # The script that will execute each time we click the button }) #endregion events } wrapping up Finally, we need to close the GUI section and invoke the ShowDialog() method to display our form\n#endregion GUI } [void]$Form.ShowDialog() The PowerShell side The GUI we have is nice, but it doing nothing. In order for the GUI to do something when we click on the button “Create Account” we need to add our PowerShell script in the button click action. But before we do it we need to write some helper functions for our script:\nGet Employee by employeeID The first function will help us get the information we need for the Manager and Creator employees, it will look for the employee ID in the Active Directory and return the user if found, or $false if not.\nfunction Get-account ($EmployeeID) { $user = Get-ADUser -Properties EmployeeID, EmailAddress -Filter {EmployeeID -eq $EmployeeID} | Select-Object Name, EmailAddress, EmployeeID if ($null -eq $user) { return $false } else { return $user } } Generate a Password Then we need to be able to generate passwords, to do that I’m using my New Password Generator core function.\n# Create a new password function new-Password() { $inputRange = 48..122 $inputRange += 33, 35 $exclude = 91..96 $exclude += 58..63 $randomRange = $inputRange | Where-Object { $exclude -notcontains $_} for ($i = 0; $i -lt 12; $i++) { $rnd = (Get-Random -InputObject $randomRange) $char = [char]$rnd $pass += $char } return $pass } Create Mail Reports Another task we need to perform is to send emails with the account information. I’m creating two emails, but it’s up to you to decide. I create one mail for the user creator and the user manager, with the account info and password. Second mail is for the security team audit, simple mail that state that account has been created, with all the information except the password.\n## Send Mail on Complition function Send-DoneEmail ($Creatoruser, $Manageruser, $NewUser, $password) { ##configure SMTP Settings $From = \u0026#34;Admin@saggiehaim.met\u0026#34; $SMTPServer = \u0026#34;saggiehaim.net\u0026#34; $SMTPPort = \u0026#34;25\u0026#34; $AdministratorsSMTPAddress = \u0026#34;contact@saggiehaim.net\u0026#34; $usersSMTPAddress = $Manageruser.EmailAddress, $Creatoruser.EmailAddress #Create email and send it to Users. $SubjectAdministrators = \u0026#34;New Service Account Has been created for you\u0026#34; $bodyToAD = \u0026#34;Hello,\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;New Service account has been created for you by: \u0026#34; + $Creator.name + \u0026#34;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service account Name:\u0026lt;/b\u0026gt; \u0026#34; + $NewUser.SamAccountName + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service account Password:\u0026lt;/b\u0026gt; \u0026#34; + $password + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;You will be asked to change the password after first logon\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;For Any further assist please contact \u0026#34; + $Creatoruser.name + \u0026#34; by Mail: \u0026#34; + $Creatoruser.EmailAddress + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;Thanks,\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;ServIT\u0026lt;br\u0026gt;\u0026#34; Send-MailMessage -From $From -to $usersSMTPAddress -Subject $SubjectAdministrators ` -Body $bodyToAD -BodyAsHtml -Encoding UTF8 -SmtpServer $SMTPServer -port $SMTPPort #Create email and send it to administrators. $SubjectAdministrators = \u0026#34;New Service Account Has been Created\u0026#34; $bodyToAD = \u0026#34;Hello,\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;New Service Account has beem created.\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service Account Name:\u0026lt;/b\u0026gt; \u0026#34; + $NewUser.name + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service Account Description:\u0026lt;/b\u0026gt; \u0026#34; + $NewUser.Description + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service Account Managed by:\u0026lt;/b\u0026gt;\u0026#34; + $NewUser.Office + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service Account Created for:\u0026lt;/b\u0026gt; \u0026#34; + $Manageruser.name + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;b\u0026gt;Service Account Created by:\u0026lt;/b\u0026gt;\u0026#34; + $Creatoruser.name + \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;\u0026lt;br\u0026gt;\u0026#34; $bodyToAD += \u0026#34;Active Directory Team\u0026lt;br\u0026gt;\u0026#34; Send-MailMessage -From $From -to $AdministratorsSMTPAddress -Subject $SubjectAdministrators ` -Body $bodyToAD -BodyAsHtml -Encoding UTF8 -SmtpServer $SMTPServer -port $SMTPPort } Create new Service account Last but not least, the reason we are here, the Service account creation function. In this function, we specify the account creation OU, the password and the rest of the attribute needed.\n#Create the Service Account function new-ADServiceAccount ($SAName, $Description, $Creatoruser, $Manageruser) { # OU to create the Service Account $path = \u0026#39;OU=Global Service Accounts,DC=Saggiehaim,DC=net\u0026#39; $password = new-Password $ManagedBy = \u0026#39;Managed By \u0026#39; + $Manageruser.Name + \u0026#39; (\u0026#39; + $Manageruser.EmployeeID + \u0026#39;)\u0026#39; try { New-ADUser -SamAccountName $SAName -UserPrincipalName $SAName -DisplayName $SAName -GivenName $SAName -Name $SAName -Description $Description -Office $ManagedBy -AccountPassword (ConvertTo-SecureString -AsPlainText $password -Force) -Enabled $true -ChangePasswordAtLogon $true -Path $path } catch { return $false } $NewUser = Get-ADUser -Identity $SAName -Properties Office, Description Send-DoneEmail $Creatoruser $Manageruser $NewUser $password return $true } The Script We built the GUI and the functions needed to perform the creation task. If you remember, when we created the GUI, we add an Action event. The script we build now is the script we want to execute each time someone presses the key.\nContent validation We will start with content validation, to make sure that the Help Desk operator is following the Service Account creation guideline.\nif ($SAccountName.Text -eq $null -or $SAccountName.Text -eq \u0026#39;\u0026#39; -or $SAccountName.Text -eq \u0026#34;Srvc\u0026#34;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Make sure you set Service Account name\u0026#34; return } elseif ($SAccountName.Text -notlike \u0026#34;Srvc*\u0026#34;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Make sure Service Account Name start with \u0026#39;Srvc\u0026#39;\u0026#34; return } if ($TDescription.Text -eq $null -or $TDescription.Text -eq \u0026#39;\u0026#39; -or $TDescription.Text -eq \u0026#34;Service Account For\u0026#34;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Make sure to insert Service Account Description!\u0026#34; return } elseif ($TDescription.Text -notlike \u0026#34;Service Account For*\u0026#34;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Start Description with \u0026#39;Service Account For\u0026#39;\u0026#34; return } if ($ManagerEmployeeID.Text -eq $null -or $ManagerEmployeeID.Text -eq \u0026#39;\u0026#39;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Enter Manager Employee ID\u0026#34; return } if ($CreatorEmployeeID.Text -eq $null -or $creatorEmployeeID.Text -eq \u0026#39;\u0026#39;) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Enter Creator Employee ID\u0026#34; return } $Creator = Get-account $CreatorEmployeeID.text $Manager = Get-account $ManagerEmployeeID.text if ($Creator -eq $false) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Validate Creator Employee ID\u0026#34; return } if ($Manager -eq $false) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Please Validate Manager Employee ID\u0026#34; return } Creating the Service Account Now after we finished with the validations, we can create the Service Account:\n$run = new-ADServiceAccount $SAccountName.text $TDescription.text $Creator $Manager if ($run -eq $false) { $StatusBar.ForeColor = \u0026#34;#ff0000\u0026#34; $StatusBar.Text = \u0026#34;Process Failed. Please verify you have permissions\u0026#34; return } else { $StatusBar.ForeColor = \u0026#34;#00b300\u0026#34; $StatusBar.Text = \u0026#34;Process Finished with no errors\u0026#34; } Conclusion We learned how to build GUI for our PowerShell scripts. How it can help us help others who don’t know yet PowerShell. And even better, give them the motivation to learn PowerShell and maybe even create their own GUI 😉\nI would love to hear your GUI’s Ideas!\nYou can see the complete script here .\n","permalink":"//localhost:8080/posts/build-gui-for-powershell/build-gui-for-powershell/","tags":["PowerShell"],"title":"Build Gui for Powershell"},{"categories":["Group Policy","PowerShell"],"content":"So, this is the first post for 2019! Happy new year!Today I want to talk about a painful topic, the Group Policy. It’s almost the same in every company. it’s historical, contain a lot of Group Policy and if you ever try to touch it, half of the systems stop working.\nIts ok burning meme Last week I saw a post from my good friend Omer a Microsoft PFE, as he talked about Common Mistakes in Active Directory and Domain Services . He points out that Removing “Authenticated Users” from the GPO Object Security Filtering is a bad thing. he also provided a PowerShell script to find GPO’s without “Authenticated Users” permissions.\nAfter running his GPO script, I saw that there few GPO’s that needed to sort. It gave me the motivation to analyze my entire environment, as a result, I created a few scripts.\nThe Target Firstly, before I start, I’m writing down what I need to look for:\nDisabled GPO’s Empty GPO’s Unlinked GPO’s Missing permissions GPO’s (Based on Omer Script) I wrote all tests together in one script, but later decided to divide them to functions, so I can run each test individually and in the future create a complete Group Policy Report utilizing those functions. Therefore let’s deep dive on each test.\nDisabled GPO’s In this test, we will look for GPO’s with User, Computer or both setting disabled.\nthe issue Technically, there is no issue with GPO with disabled settings. But if both of them is disabled, then its unused GPO. We probably don’t need them. If only User or Computer disabled, then it\u0026rsquo;s a “red flag” and we need to make sure we are not “losing” configuration. Maybe we can merge the settings with another GPO?\nThe Function This will check if there are any settings disabled and return the GPO\u0026rsquo;s.\nfunction Get-GPODisabled { [cmdletbinding()] param ( ) try { Write-Verbose -Message \u0026#34;Importing GroupPolicy module\u0026#34; Import-Module GroupPolicy -ErrorAction Stop } catch { Write-Error -Message \u0026#34;GroupPolicy Module not found. Make sure RSAT (Remote Server Admin Tools) is installed\u0026#34; exit } $DisabledGPO = New-Object System.Collections.ArrayList try { Write-Verbose -Message \u0026#34;Importing GroupPolicy Policies\u0026#34; $GPOs = Get-GPO -All Write-Verbose -Message \u0026#34;Found \u0026#39;$($GPOs.Count)\u0026#39; policies to check\u0026#34; } catch { Write-Error -Message \u0026#34;Can\u0026#39;t Load GPO\u0026#39;s Please make sure you have connection to the Domain Controllers\u0026#34; exit } ForEach ($gpo in $GPOs) { Write-Verbose -Message \u0026#34;Checking \u0026#39;$($gpo.DisplayName)\u0026#39; status\u0026#34; switch ($gpo.GpoStatus) { \u0026#34;ComputerSettingsDisabled\u0026#34; {$DisabledGPO += \u0026#34;in \u0026#39;$($gpo.DisplayName)\u0026#39; the Computer Settings Disabled\u0026#34;} \u0026#34;UserSettingsDisabled\u0026#34; {$DisabledGPO += \u0026#34;in \u0026#39;$($gpo.DisplayName)\u0026#39; the User Settings Disabled\u0026#34;} \u0026#34;AllSettingsDisabled\u0026#34; {$DisabledGPO += \u0026#34;in \u0026#39;$($gpo.DisplayName)\u0026#39; All Settings Disabled\u0026#34;} } } if (($DisabledGPO).Count -ne 0) { Write-Host \u0026#34;The Following GPO\u0026#39;s have settings disabled:\u0026#34; return $DisabledGPO } else { return \u0026#34;No GPO\u0026#39;s with setting disabled Found\u0026#34; } } PowerShell Terminal with the result of running the Get-GPODisabled Empty GPO’s In this test, we will look for empty GPO’s, that means that the GPO has no setting configured on it.\nThe issue Empty GPO’s are useless and just adding more overhead. We don’t need them in our environment.\nThe Function This function will check if there is GPO with the User or Computers settings empty. one common mistake people do, is they check the “User version” and “Computer version”, while it true that a new GPO start with version 0 on both of them and changed in each modification. but if you take an old GPO and remove all settings, the version won’t reset to 0, and you will miss those GPO’s!\nExample of GPO settings function Get-GPOEmpty { [cmdletbinding()] param ( ) try { Write-Verbose -Message \u0026#34;Importing GroupPolicy module\u0026#34; Import-Module GroupPolicy -ErrorAction Stop } catch { Write-Error -Message \u0026#34;GroupPolicy Module not found. Make sure RSAT (Remote Server Admin Tools) is installed\u0026#34; exit } $EmptyGPO = New-Object System.Collections.ArrayList try { Write-Verbose -Message \u0026#34;Importing GroupPolicy Policies\u0026#34; $GPOs = Get-GPO -All Write-Verbose -Message \u0026#34;Found \u0026#39;$($GPOs.Count)\u0026#39; policies to check\u0026#34; } catch { Write-Error -Message \u0026#34;Can\u0026#39;t Load GPO\u0026#39;s Please make sure you have connection to the Domain Controllers\u0026#34; exit } ForEach ($gpo in $GPOs) { Write-Verbose -Message \u0026#34;Checking \u0026#39;$($gpo.DisplayName)\u0026#39; status\u0026#34; \u0026amp;#91;xml]$GPOXMLReport = $gpo | Get-GPOReport -ReportType xml if ($null -eq $GPOXMLReport.gpo.User.ExtensionData -and $null -eq $GPOXMLReport.gpo.Computer.ExtensionData) { $EmptyGPO += $gpo } } if (($EmptyGPO).Count -ne 0) { Write-Host \u0026#34;The Following GPO\u0026#39;s are empty:\u0026#34; return $EmptyGPO.DisplayName } else { return \u0026#34;No Empty GPO\u0026#39;s Found\u0026#34; } } PowerShell Terminal with the result of running the Get-GPOEmpty Unlinked GPO’s In this test, we will have the most results, as it\u0026rsquo;s the most common case. Unlinked GPO’s are GPO’s that not linked to any domain, site or OU.\nThe issue Unlinked GPO’s are not an “issue” but most of the time, they are useless overhead to manage and track. In my company, there was GPO’s from 2008. I guess we won’t use it anymore 😒\nThe function function Get-GPOUnlinked { [cmdletbinding()] param () try { Write-Verbose -Message \u0026#34;Importing GroupPolicy module\u0026#34; Import-Module GroupPolicy -ErrorAction Stop } catch { Write-Error -Message \u0026#34;GroupPolicy Module not found. Make sure RSAT (Remote Server Admin Tools) is installed\u0026#34; exit } $UnlinkedGPO = New-Object System.Collections.ArrayList try { Write-Verbose -Message \u0026#34;Importing GroupPolicy Policies\u0026#34; $GPOs = Get-GPO -All Write-Verbose -Message \u0026#34;Found \u0026#39;$($GPOs.Count)\u0026#39; policies to check\u0026#34; } catch { Write-Error -Message \u0026#34;Can\u0026#39;t Load GPO\u0026#39;s Please make sure you have connection to the Domain Controllers\u0026#34; exit } ForEach ($gpo in $GPOs) { Write-Verbose -Message \u0026#34;Checking \u0026#39;$($gpo.DisplayName)\u0026#39; link\u0026#34; \u0026amp;#91;xml]$GPOXMLReport = $gpo | Get-GPOReport -ReportType xml if ($null -eq $GPOXMLReport.GPO.LinksTo) { $UnlinkedGPO += $gpo } } if (($UnlinkedGPO).Count -ne 0) { return $UnlinkedGPO.DisplayName } else { return Write-Host \u0026#34;No Unlinked GPO found\u0026#34; } } PowerShell Terminal with the result of running the Get-GPOUnlinked Missing Permissions Here we will test to see if there are GPO’s with missing Read permissions to the “Authenticated Users” or “Domain Computers” groups.\nThe Issue This test is based on Omer post I shared before, so I recommend you give it a read if you haven’t yet. he explains it better than me 😃\nThe Function function Get-GPOMissingPermissions { [cmdletbinding()] param () try { Write-Verbose -Message \u0026#34;Importing GroupPolicy module\u0026#34; Import-Module GroupPolicy -ErrorAction Stop } catch { Write-Error -Message \u0026#34;GroupPolicy Module not found. Make sure RSAT (Remote Server Admin Tools) is installed\u0026#34; exit } $MissingPermissionsGPOArray = New-Object System.Collections.ArrayList try { Write-Verbose -Message \u0026#34;Importing GroupPolicy Policies\u0026#34; $GPOs = Get-GPO -All Write-Verbose -Message \u0026#34;Found \u0026#39;$($GPOs.Count)\u0026#39; policies to check\u0026#34; } catch { Write-Error -Message \u0026#34;Can\u0026#39;t Load GPO\u0026#39;s Please make sure you have connection to the Domain Controllers\u0026#34; exit } ForEach ($gpo in $GPOs) { Write-Verbose -Message \u0026#34;Checking \u0026#39;$($gpo.DisplayName)\u0026#39; status\u0026#34; If ($GPO.User.Enabled) { $GPOPermissionForAuthUsers = Get-GPPermission -Guid $GPO.Id -All | Select-Object -ExpandProperty Trustee | Where-Object {$_.Name -eq \u0026#34;Authenticated Users\u0026#34;} $GPOPermissionForDomainComputers = Get-GPPermission -Guid $GPO.Id -All | Select-Object -ExpandProperty Trustee | Where-Object {$_.Name -eq \u0026#34;Domain Computers\u0026#34;} If (!$GPOPermissionForAuthUsers -and !$GPOPermissionForDomainComputers) { $MissingPermissionsGPOArray += $gpo } } } if (($MissingPermissionsGPOArray).Count -ne 0) { Write-Host \u0026#34;The following GPO\u0026#39;s do not grant any permissions to the \u0026#39;Authenticated Users\u0026#39; Or \u0026#39;Domain Computers\u0026#39; Group\u0026#34; return $MissingPermissionsGPOArray.DisplayName } else { return \u0026amp;#91;string]\u0026#34;No GPO\u0026#39;s with missing permissions to the \u0026#39;Authenticated Users\u0026#39; or \u0026#39;Domain Computers\u0026#39; groups found \u0026#34; } } PowerShell Terminal with the result of running the Get-GPOMissingPermissions Doing it Safe Before you make any changes, I highly recommend you to backup any GPO before you change or delete. If you hear people scream and shout outside, just restore the GPO.\nBackup GPO\u0026rsquo;s While you can easily backup GPO’s over the GUI, we love PowerShell more. so we will do it with PowerShell.\nTo backup all GPO’s or specific ones, just run the following cmdlet:\n## Backup All GPO\u0026#39;s Backup-Gpo -All -Path \u0026#34;Location\u0026#34; ## Backup specific GPO Backup-Gpo -Name \u0026#34;GPO NAME\u0026#34; -Path \u0026#34;Location\u0026#34; -Comment \u0026#34;Comment\u0026#34; Restore-GPO If we want to restore deleted or changed GPO, we can use the following cmdlet:\n## Restore All GPO\u0026#39;s Restore-GPO -All -Domain \u0026#34;Domain Name\u0026#34; -Path \u0026#34;Backups Location\u0026#34; ## Restore specific GPO Restore-GPO -Name \u0026#34;GPO NAME\u0026#34; -Path \u0026#34;Backup Location Restore-GPO only support restoring GPO’s of the same domain!\nConclusion In conclusion, we learned what we need to search. We saw the functions that will help us find the GPO’s. And we learned to backup and restore GPO’s. It’s time to sort the group policy mess! in the future, I hope to release the full GPO environment report, so stay tuned and good luck!\nYou can find all the code on GitHub ","permalink":"//localhost:8080/posts/sorting-group-policy/sorting-group-policy/","tags":["Group Policy","PowerShell","Automation"],"title":"Sorting Group Policy"},{"categories":["SharePoint","PowerShell"],"content":"Hey Guys! It’s been a long time since my last post, as I been busy with work trips. But now I’m back! And I want to share with you my experience with PowerShell and SharePoint Online. To be honest, I never worked with SharePoint before. I got a request from the SharePoint team to update a list weekly from a CSV report that HR provides (Weekly employees birthdays, nothing exciting).\nYou have no power here meme First thing to do? Jump head-on to google and understand what my tools are. I found this task to be really easy, but it took me a while to find what I needed. Every post I sew online talked about the SharePoint Online module, and some complex queries and scripts\u0026hellip; But I share the greatest skill of IT Admin, I’m lazy\u0026hellip; and it seems hell lot of work for such a simple task\u0026hellip; So, I move on and keep searching.\nAfter reading Microsoft Docs I notice something called PnP, Jackpot! PnP stands for Patterns and Practices, and it contains a library of PowerShell commands that allows you to perform complex provisioning and artifact management actions towards SharePoint.\nSo, I first looked for my “lists” cmdlets and found everything I need to complete my tasks. I won’t write much about installing and using this module as Microsoft Docs provides a great explanation, you can visit the page here .\nSo, to further explain my task, I wrote my steps:\nConnect to the SharePoint Online Import the CSV Empty the list Update the list from the CSV Connect to the SharePoint Online To connect your SharePoint online using the PnP is really easy:\n$userCredential = Get-Credential -Message \u0026#34;Type the password.\u0026#34; $WebUrl = \u0026#34;https://SaggieHaim.sharepoint.com/sites/Portal\u0026#34; Connect-PnPOnline –Url $WebUrl –Credentials $userCredential Import the CSV After we connected to the SharePoint, it’s pretty easy to import the CSV:\n$employeesBirthday = Import-Csv -Path \u0026#34;D:\\Tasks\\Update Sharepoint Events list\\WeeklyBirthdays.CSV\u0026#34; Empty the list Now to empty the list, we can simply use the Get-PnPListItem and Remove-PnPListItem:\nGet-PnPListItem -List \u0026#34;$listName\u0026#34; | foreach { Remove-PnPListItem -List \u0026#34;$listName\u0026#34; -Identity $_.Id -Force} Update the list from the CSV So, we passed the halfway mark, but this step gave me a headache! For some reason I kept getting this message when I tried to update to list:\nadd-pnplistItem : The specified user could not be found”\nAnd I’m like, WHAT?? it\u0026rsquo;s a string! not a username!\nI decided to try one manually just to understand the fields in the list, and then I notice, that EventAuthor field request a username! The report I get from the old HR system contains the full name, and luckily for me, it’s enough. But, unlucky for me… I get Last Name First Name, and SharePoint wants First Name Last Name 🤦‍♀️ so off we go:\nWe can use the add-PnPListItem cmdlet for each employee in the list:\nforeach ($employee in $employeesBirthday) { ## Replacing first and last names $EventAuthor = $employee.name.Split(\u0026#34; \u0026#34;)[1] + \u0026#34; \u0026#34; + $employee.name.Split(\u0026#34; \u0026#34;)[0] $expire = $person.Expires Add-PnPListItem -List \u0026#34;LumenisGreetingsList\u0026#34; -Values @{ \u0026#34;Title\u0026#34; = \u0026#34;IMF\u0026#34; ; \u0026#34;EventDay\u0026#34; = \u0026#34;$employee.EventDay\u0026#34;; \u0026#34;EventMonth\u0026#34; = \u0026#34;$employee.EventMonth\u0026#34;; \u0026#34;EventType\u0026#34; = \u0026#34;birthday\u0026#34;; \u0026#34;Role\u0026#34; = \u0026#34;$employee.Role\u0026#34;; \u0026#34;EventAuthor\u0026#34; = $EventAuthor; \u0026#34;Expires\u0026#34; = \u0026#34;$employee.expire\u0026#34; } } Now that everything is working, tested and ready I can build my first SharePoint and PowerShell automation script 😁 I hope my hard learning will make it easier for your “first time” with SharePoint Online!\n","permalink":"//localhost:8080/posts/updating-sharepoint-online-list-with-powershell-the-easy-way/updating-sharepoint-online-list-with-powershell-the-easy-way/","tags":["SharePoint","PowerShell","Automation"],"title":"Updating Sharepoint Online List With Powershell the Easy Way"},{"categories":["PowerShell"],"content":"Howdy folks! How are you today?\nAfter my last post, I had to prepare for my trip to the USA, had to visit two of our offices and set up a new one. Unfortunately, the big California fire started… so things ware little tense during my stay, hope everyone is safe over there!\nWhile working there, my colleague asks me about some cmdlets I ran in the last site we visited. I had a few PowerShell Sessions open, and I wasn’t sure in which one I ran the cmdlets, so I had to bring my investigator glasses.\nbad eel joke meme So, let’s talk about PowerShell history. Technically, PowerShell has two type of commands history. One is the command line buffer, which is part of the graphical terminal application. Second is the built-in command history feature, that provides detailed information about the commands you have run.\nSo, what are the differences? Command Line Buffer By default, the buffer remembers the last 50 commands typed in. Also, as we said before because is part of the graphical terminal application, it’s not limited to the current PowerShell session (YES!). that means that every time we open a new session, we will have the same buffer, but know that if you already have two sessions running, you will have to reopen one of them in order to refresh the buffer.\nHow to use? Up and Down keys: Navigate thru the buffer list, pressing the Up key will recall the last command you type, continue pressing the up key to move further in the list. Pressing the Down key will move the other way on the list. F8: one of the coolest features in my opinion. You can’t view your buffer list, which makes it harder to find what you want, but with the help of F8, we can search thru the buffer. just type in the cmdlet you are looking for and press F8. Not what you looked for? press F8 again to further search the list.\nhow to change the buffer size? We can increase the size of the buffer easily if we want, but pressing the right click on the PowerShell Title Bar, select “Defaults”, under the “Options” tab, change the value under “Buffer Size”\nPowerShell Terminal buffer setting PowerShell History Cmdlet Windows PowerShell itself keeps a history of the commands you have typed in. but this time its unique to the current session. this means that unlike the buffer, it won’t be shared across sessions, and won’t save after you close the session. the default history size is determined by the parameter $MaximumHistoryCount (from version 3.0 value set to 4096)\nhow to use? Get-History To view the command line history, use the following cmdlet:\nGet-History We can also use the “h” alias if you want. Each cmdlet gets an ID in the order they executed. We can recall specific cmdlet from the history with it ID.\nGet-History -Id 3 cool tip: If we need to copy cmdlet from history to somewhere else, you can pipe it to “clip” and it will be ready in your clipboard.\n(Get-History -Id 1).CommandLine | clip To search the history, we can use the Select-String cmdlet:\nGet-History | Select-String \u0026#34;Saggiehaim\u0026#34; If you need to see detailed information about each command you execute, you can use the Format-List:\nGet-History | Format-List -Property * If we need to save our history list for future purposes, we can export the list to CSV or XML files. When we export the history, the file includes the data that displayed when we format the history as a list, including all the details.\nGet-History | Export-Csv History.csv Add-History We can use the Get-History cmdlet to import the history list we exported.\nImport-Csv history.csv | Add-History Clear History If we need for some reasons to clear the history and start clean, we can do run the following cmdlet:\nClear-History We can also delete specific cmdlet from history, using:\nClear-History -Id # Invoke History If we want to re-run a command from the history list, we can use the invoke history cmdlet. First, we need to check the ID of the cmdlet we want to run from history, then we can use the following:\nInvoke-History -Id # PowerShell Terminal running the Invoke-History If needed, we can run more than one cmdlet:\n## Choosing specific cmdlets 1,5,7 | ForEach {Invoke-History -Id $_ } ## Running a sequence 10..20 | ForEach {Invoke-History -Id $_ } Combining everything So, we can view the PowerShell history, export it, import it and run cmdlets from the list. That nice, but we can take it one step ahead. We worked in a session, made some trials and errors. and ended with a collection of cmdlets that fit our needs, it\u0026rsquo;s not a script material, but it\u0026rsquo;s a sequence of cmdlets we need to complete our task. We can use the history cmdlets to our advantage. We can export the PowerShell history list and import it to any session we need, then we can invoke the cmdlets one by one. creating a “script” in seconds:\nImport-Csv history.csv | Add-History -PassThru | ForEach-Object -Process {Invoke-History} Summary Now that we know how to search the PowerShell history and use the buffer. We can be much more productive! I encourage you to try and explore all the cmdlets we talked about here. get familiar with it and add it to your skills set.\nSee you next time!\n","permalink":"//localhost:8080/posts/powershell-history/powershell-history/","tags":["PowerShell"],"title":"PowerShell History"},{"categories":["PowerShell","VMware"],"content":"Konichiwa Yujin,\nSorry for not posting for so long, I been to Japan for a work trip. While Japan is amazing, the state of the ESXi in the office wasn’t so great. The ESXi crushed, and after investigation, we notice that he had no free space on HDD’s, which was weird. we have 100TB, and only 4 VM’s. the first suspect? Snapshots!\nJapanese Garden building Snapshots are great, but they can be your worst enemy if you don’t use them correctly. The “issue” with snapshots is that they save your delta’s. This means that every change happening on the VM is “recorded” on the snapshot, “another change to revert”. As the time pass, the snapshot will grow and grow with the limit set to your free space on HDD’s.\nIt easy to set up a snapshot, but it hard to track them, especially when you have a big environment and many “hands” on the system. but we have an easy solution, PowerShell. We can create a report, that give us all the information needed to manage the snapshots, so let get started!\nPrerequisites Before we can start, please make sure you have the following:\nAccess to the VCenter or ESXi (I encourage to create a service account with relevant permissions) A server or PC with PowerCLI installed. Step 1 – Install PowerCLI Installing PowerCLI is easy, we can get it directly from the PowerShell Gallery:\nInstall-Module -Name VMware.PowerCLI -Scope CurrentUser Terminal Window with PowerCLI installation Step 2 – Connect to the VCenter or ESXi Before we can run any command in PowerCLI, we need to connect the VCenter or ESXi. To do so we need to use the Connect-VIServer cmdlet:\nConnect-VIServer -Server myvc01 -User \u0026#39;Username\u0026#39; -Password \u0026#39;Password\u0026#39; If you have any issue with the certificate, you will get a notice now. it will be in yellow which state warning and not an error. I advise you to fix it, but it’s not necessary for our cause.\nA little tip, keeping the password in plain text in scripts is not so safe, so it’s better to secure it a little bit. I don’t like this method either, but this is much safer. First, we will convert the password to secure string, and save it to a file (I recommend changing the ACL for the file also).\n$PasswordFile = \u0026#34;\u0026lt;Path\u0026gt;\\PasswordFile.txt\u0026#34;Read-Host -AsSecureString | ConvertFrom-SecureString | Out-File $CredsFile Now we can connect to the VCenter or ESXi more secure:\n$PasswordFile = \u0026#34;\u0026lt;Path\u0026gt;\\PasswordFile.txt\u0026#34;$securePassword = Get-Content $PasswordFile | ConvertTo-SecureString$credentials = New-Object System.Management.Automation.PSCredential (\u0026#34;Username\u0026#34;, $securePassword)Connect-VIServer -Server myvc01 -Credential $credentials Please, do yourself a favor and use this method in your script.\nStep 3 – Get all the Snapshots Snapshots are unique to a specific VM, so first we need to get all the VM’s on the Vcenter or ESXi, to do it we will use the Get-VM cmdlet, then for each VM we will get the snapshots, we will use the Get-Snapshot cmdlet.\n$Snapshots = Get-VM | Get-Snapshot | select Description,Created,VM,SizeMB,SizeGB I only took the properties that I needed.\nStep 4 – Process the Data Now, I’ll be honest with you, you can pipe this to Format-List and you will have a great report to start with. But I always love to make my reports better looking. So, there are a few things we need to process first.\nSnapshot Size I will create a simple function that will process the snapshot size (this why I took both SizeMB and SizeGB) and return a nice value.\nfunction Get-SnapshotSize ($Snapshot) { if ($snapshot.SizeGB -ge \u0026#34;1\u0026#34;) { $Snapshotsize = [string]([math]::Round($snapshot.SizeGB, 3)) + \u0026#34; GB\u0026#34; } else { $Snapshotsize = [string]([math]::Round($snapshot.SizeMB, 3)) + \u0026#34; MB\u0026#34; } Return $Snapshotsize } Snapshot Created Date I also want to add some colors to my reports, Convertto-HTML cmdlet doesn’t support conditional formation, so I will process the Created date to style it manually, I decided that snapshots not older than 1 week will be green, snapshots between 1 to 2 weeks will be yellow, and snapshots older than 2 weeks will be red.\nfunction set-SnapshotDate ($snapshot) { $greenValue = (get-date).AddDays(-7) $RedValue = (get-date).AddDays(-14) if ($snapshot.created -gt $greenValue) { $backgroundcolor = \u0026#34;green\u0026#34; } elseif ($snapshot.Created -lt $greenValue -and $snapshot.Created -gt $RedValue) { $backgroundcolor = \u0026#34;yellow\u0026#34; } else { $backgroundcolor = \u0026#34;red\u0026#34; } return $backgroundcolor } Then, I created another function that will get the HTML data and change the style.\nfunction Format-HTMLBody ($body) { $newbody = @() foreach ($line in $body) { ## Remove the Format Header if ($line -like \u0026#34;*\u0026lt;th\u0026gt;Format\u0026lt;/th\u0026gt;*\u0026#34;) { $line = $line -replace \u0026#39;\u0026lt;th\u0026gt;Format\u0026lt;/th\u0026gt;\u0026#39;,\u0026#39;\u0026#39; } ## Format all the Red rows if ($line -like \u0026#34;*\u0026lt;td\u0026gt;red\u0026lt;/td\u0026gt;*\u0026#34;) { $line = $line -replace \u0026#39;\u0026lt;td\u0026gt;red\u0026lt;/td\u0026gt;\u0026#39;,\u0026#39;\u0026#39; $line = $line -replace \u0026#39;\u0026lt;tr\u0026gt;\u0026#39;,\u0026#39;\u0026lt;tr style=\u0026#34;background-color:Tomato;\u0026#34;\u0026gt;\u0026#39; } ## Formating all the Yellow Rows elseif ($line -like \u0026#34;*\u0026lt;td\u0026gt;yellow\u0026lt;/td\u0026gt;*\u0026#34;) { $line = $line -replace \u0026#39;\u0026lt;td\u0026gt;yellow\u0026lt;/td\u0026gt;\u0026#39;,\u0026#39;\u0026#39; $line = $line -replace \u0026#39;\u0026lt;tr\u0026gt;\u0026#39;,\u0026#39;\u0026lt;tr style=\u0026#34;background-color:Orange;\u0026#34;\u0026gt;\u0026#39; } ## Formating all the Green Rows elseif ($line -like \u0026#34;*\u0026lt;td\u0026gt;green\u0026lt;/td\u0026gt;*\u0026#34;) { $line = $line -replace \u0026#39;\u0026lt;td\u0026gt;green\u0026lt;/td\u0026gt;\u0026#39;,\u0026#39;\u0026#39; $line = $line -replace \u0026#39;\u0026lt;tr\u0026gt;\u0026#39;,\u0026#39;\u0026lt;tr style=\u0026#34;background-color:MediumSeaGreen;\u0026#34;\u0026gt;\u0026#39; } ## Building the new HTML file $newbody += $line } return $newbody } This is ungentle solution, if you have a better idea, I would love to here!\nStep 3 – Create the Snapshots Report Now that we have all the data we need, we can create the report. We will create an HTML report, so it will be easy to use it in different scenarios.\nHeader and Style First, we will handle the HTML header and set basic style to our page.\n$date = (get-date -Format d/M/yyyy) $header =@\u0026#34; \u0026lt;Title\u0026gt;Snapshot Report - $date\u0026lt;/Title\u0026gt; \u0026lt;style\u0026gt; body { font-family: \u0026#39;Helvetica Neue\u0026#39;, Helvetica, Arial; font-size: 14px; line-height: 20px; font-weight: 400; color: black; } table{ margin: 0 0 40px 0; width: 100%; box-shadow: 0 1px 3px rgba(0,0,0,0.2); display: table; border-collapse: collapse; border: 1px solid black; } th { font-weight: 900; color: #ffffff; background: black; } td { border: 0px; border-bottom: 1px solid black } \u0026lt;/style\u0026gt; \u0026#34;@ Create Title to Our Table To add a title before our table:\n$PreContent = \u0026#34;\u0026lt;H1\u0026gt; Snapshot Report for \u0026#34; + $date + \u0026#34;\u0026lt;/H1\u0026gt;\u0026#34; Convert to HTML We did everything needed to create our report, now let’s wrap up everything.\n$html = $Snapshots | Select-Object VM, Created, @{Label = \u0026#34;Size\u0026#34;; Expression = { Get-SnapshotSize($_) } }, Description, @{Label = \u0026#34;Format\u0026#34;; Expression = { set-SnapshotDate($_) } } | Sort-Object Created -Descending | ConvertTo-Html -Head $header -PreContent $PreContent Format the HTML with Conditional Formatting $Report = Format-HTMLBody ($html) Step 4 – Publish the Snapshots Report now that we have our report ready, we need to decide how to publish it. either by mail or as an HTML page, you choose.\nImage of the report Create HTML File If you want to create an HTML file, just pipe your report to out-file\n$report | out-file -Path \u0026lt;path\u0026gt; Send Report Over Mail To send the report over mail, we can use the Send-MailMessage cmdlet\n$MailParam = @{ To = \u0026#34;SaggieHaim@saggiehaim.net\u0026#34; From = \u0026#34;VCenter@saggiehaim.net\u0026#34; SmtpServer = \u0026#34;10.0.0.4\u0026#34; Subject = \u0026#34;VMware Snapshot Report for \u0026#34; + (get-date -Format d/M/yyyy) body = ([string]$Report) } Send-MailMessage @MailParam -BodyAsHtml Step 5 – Disconnect from Server It’s very important to disconnect from the server when we finish. Two main reasons, one for security reasons, and two for session limit on VMWare, the limit is set to 100 concurrent sessions, both Idle and active sessions count. we can disconnect from the server with the Disconnect-VIServer\nDisconnect-VIServer -Confirm:$false Conclusion Snapshots are great! but they can do harm! We learned how to generate snapshot reports, to see how old the snapshots are and how much storage they take. Decide the frequency you want to run this report, and set a scheduled task, but remember to be secure! I will also include the full script if you want, you can download it here See you next time!\n","permalink":"//localhost:8080/posts/create-vmware-snapshots-report/create-vmware-snapshots-report/","tags":["PowerShell","VMware","Automation"],"title":"Create Vmware Snapshots Report"},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"//localhost:8080/search/_index.zh-cn/","tags":null,"title":""}]